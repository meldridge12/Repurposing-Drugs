================================================================================
IMPROVED XGBOOST DRUG REPURPOSING MODEL - COMPREHENSIVE REPORT
================================================================================
Generated: 2025-12-03 20:27:36
================================================================================

KEY IMPROVEMENTS IMPLEMENTED:
--------------------------------------------------------------------------------
1. ✓ Balanced class sampling (capped at 800 samples per disease)
2. ✓ Class weights to handle remaining imbalance
3. ✓ Increased regularization (reduced overfitting)
4. ✓ Enhanced feature engineering (6 new features)
5. ✓ Top-K accuracy metrics for multi-class evaluation
6. ✓ Lower learning rate with more estimators

1. DATASET INFORMATION
--------------------------------------------------------------------------------
Initial rows loaded:           300,000
Initial unique diseases:       3728
Sample range per disease:      150-800
Rows after balanced sampling:  189,057
Final unique diseases:         576
Data memory usage:             18.83 MB

Disease Distribution Statistics (After Balancing):
  Mean samples per disease:    328.22
  Median samples per disease:  291.50
  Std deviation:               166.80
  Min samples:                 150
  Max samples:                 800
  Imbalance ratio:             5.33:1

Top 10 Most Common Diseases:
   1. Urinary Bladder Neoplasms                                         800 samples
   2. Weight Gain                                                       800 samples
   3. Liver Diseases                                                    800 samples
   4. Kidney Failure, Chronic                                           800 samples
   5. Lung Injury                                                       800 samples

2. PREPROCESSING & FEATURE ENGINEERING
--------------------------------------------------------------------------------
Original features:             10
Engineered features:           6
Total features:                16
Total samples:                 189,057
Number of classes:             576

Missing Values Imputed:
  inferencescore                    1,201 missing values

Engineered Features:
  1. logp_mean         (average of logp_alogps and logp_chemaxon)
  2. h_bond_total      (sum of n_hba and n_hbd)
  3. pka_range         (difference between pka acidic and basic)
  4. h_bond_ratio      (donor/acceptor ratio)
  5. mw_per_hbond      (molecular weight per H-bond)
  6. logp_variance     (consistency measure across logp methods)

3. MODEL CONFIGURATION
--------------------------------------------------------------------------------
Improved XGBoost Parameters (Anti-Overfitting):
  objective                      multi:softprob
  num_class                      576
  max_depth                      4
  learning_rate                  0.05
  n_estimators                   150
  subsample                      0.7
  colsample_bytree               0.7
  min_child_weight               10
  gamma                          0.3
  reg_alpha                      0.5
  reg_lambda                     2.0
  random_state                   42
  n_jobs                         4
  tree_method                    hist
  max_bin                        256
  eval_metric                    mlogloss
  early_stopping_rounds          15

Best iteration:                149
Class weights applied:         True

4. MODEL PERFORMANCE
--------------------------------------------------------------------------------
Training Accuracy:             0.455684
Testing Accuracy (Top-1):      0.226886
Testing Accuracy (Top-3):      0.777981
Testing Accuracy (Top-5):      0.954565
Testing Accuracy (Top-10):     0.997488
Testing Precision (weighted):  0.229605
Testing Recall (weighted):     0.226886
Testing F1-Score (weighted):   0.207171

Overfitting Gap:               0.228799 (22.88%)

Training History:
  Initial train loss:          5.936225
  Final train loss:            1.250968
  Initial test loss:           5.937815
  Final test loss:             1.473339
  Train loss reduction:        4.685257
  Test loss reduction:         4.464476

5. FEATURE IMPORTANCE
--------------------------------------------------------------------------------
Top 15 Most Important Features:
   1. inferencescore                 0.72134072
   2. logp_mean                      0.04798819
   3. logp_variance                  0.03566572
   4. logp_alogps                    0.02136878
   5. logp_chemaxon                  0.01985075
   6. logp                           0.01578771
   7. mw_per_hbond                   0.01456045
   8. pka__strongest_basic_          0.01453626
   9. pka__strongest_acidic_         0.01423894
  10. pka_range                      0.01413643
  11. molecular_weight               0.01376739
  12. h_bond_total                   0.01364790
  13. n_hbd                          0.01355169
  14. ro5_fulfilled                  0.01332248
  15. h_bond_ratio                   0.01317315

6. CLASSIFICATION REPORT (Top 10 Diseases)
--------------------------------------------------------------------------------
                           precision    recall  f1-score   support

          Liver Cirrhosis       0.00      0.00      0.00       160
              Lung Injury       0.00      0.00      0.00       160
            Lung Diseases       1.00      0.01      0.01       160
          Liver Neoplasms       0.00      0.00      0.00       160
  Kidney Failure, Chronic       0.00      0.00      0.00       160
       Insulin Resistance       1.00      0.17      0.29       160
      Infertility, Female       0.00      0.00      0.00       160
              Weight Gain       1.00      0.47      0.64       160
            Stomach Ulcer       1.00      0.03      0.05       160
Urinary Bladder Neoplasms       0.00      0.00      0.00       160

                micro avg       1.00      0.07      0.13      1600
                macro avg       0.40      0.07      0.10      1600
             weighted avg       0.40      0.07      0.10      1600


7. OUTPUT FILES GENERATED
--------------------------------------------------------------------------------
Visualizations:
  1. disease_distribution.png      - Balanced disease distribution
  2. feature_distributions.png     - Feature histograms
  3. confusion_matrix.png          - Top 10 diseases confusion matrix
  4. prediction_distribution.png   - Actual vs predicted
  5. metrics_comparison.png        - Performance metrics
  6. topk_accuracy.png             - Top-K accuracy visualization
  7. feature_importance.png        - Feature importance scores
  8. training_history.png          - Loss curves and overfitting
  9. performance_summary.png       - Comprehensive dashboard

Model Files:
  1. improved_xgboost_model.json   - Trained model
  2. improved_model_report.txt     - This report

8. ANALYSIS & INSIGHTS
--------------------------------------------------------------------------------
⚠ Model still shows overfitting (train > test by 22.88%)
  - Regularization has been increased
  - Consider further reducing max_depth or increasing min_child_weight

✓ Strong Top-3 accuracy (77.80%)
  Model provides useful top-3 predictions for drug repurposing
✓ Strong Top-5 accuracy (95.46%)
  Model can narrow down to 5 candidate diseases effectively

⚡ Moderate class imbalance remains (ratio: 5.33:1)

✓ Most important feature: inferencescore (0.7213)
  This feature dominates disease prediction

9. RECOMMENDATIONS FOR FURTHER IMPROVEMENT
--------------------------------------------------------------------------------
To improve Top-1 accuracy:
  • Add more domain-specific features (binding affinity, SMILES-based)
  • Use ensemble methods (combine multiple models)
  • Consider hierarchical classification (group similar diseases)
  • Experiment with neural networks for feature learning

Multi-class strategies:
  • Focus on Top-K metrics for practical use cases
  • Consider one-vs-rest classifiers for critical diseases
  • Use confidence thresholds to reject uncertain predictions
  • Implement active learning for difficult cases

Data collection recommendations:
  • Collect more samples for underrepresented diseases
  • Include protein target information if available
  • Add pathway and mechanism data
  • Consider temporal information (disease progression)

10. CONCLUSION
--------------------------------------------------------------------------------
The improved XGBoost model was trained on 189,057 balanced samples
across 576 diseases using 16 features.

Key achievements:
  • Reduced overfitting from 25% to 22.9%
  • Achieved 22.69% Top-1 accuracy
  • Achieved 95.46% Top-5 accuracy
  • Implemented class balancing and weights
  • Added 6 engineered features

For multi-class drug repurposing with 576 diseases, Top-K metrics
are most relevant. The model successfully narrows candidates from 576
to 5-10 diseases with 95.5% accuracy, providing actionable insights
for drug repurposing research.

================================================================================
END OF IMPROVED MODEL REPORT
================================================================================