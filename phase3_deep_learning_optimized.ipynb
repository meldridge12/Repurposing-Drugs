{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d0f1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, top_k_accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbd50a6",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa80f87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('finaldataset.csv', on_bad_lines='skip', engine='python')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Unique diseases: {df['diseasename'].nunique()}\")\n",
    "print(f\"Unique chemicals: {df['chemicalname'].nunique()}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a0b52e",
   "metadata": {},
   "source": [
    "## 2. Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b3346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF from descriptions (increase to 200 features for more signal)\n",
    "df['description_clean'] = df['description'].fillna('')\n",
    "tfidf = TfidfVectorizer(max_features=200, stop_words='english', min_df=3, ngram_range=(1, 2))\n",
    "tfidf_matrix = tfidf.fit_transform(df['description_clean'])\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(), \n",
    "    columns=[f'tfidf_{i}' for i in range(tfidf_matrix.shape[1])]\n",
    ")\n",
    "\n",
    "print(f\"TF-IDF features: {tfidf_df.shape[1]} (with bigrams)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f2af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMILES features\n",
    "def extract_smiles_features(smiles_str):\n",
    "    if pd.isna(smiles_str) or smiles_str == '':\n",
    "        return {\n",
    "            'smiles_length': 0,\n",
    "            'smiles_num_atoms': 0,\n",
    "            'smiles_num_bonds': 0,\n",
    "            'smiles_num_rings': 0,\n",
    "            'smiles_has_aromatic': 0,\n",
    "            'smiles_num_branches': 0,\n",
    "            'smiles_num_double_bonds': 0,\n",
    "            'smiles_num_triple_bonds': 0,\n",
    "        }\n",
    "    \n",
    "    s = str(smiles_str)\n",
    "    return {\n",
    "        'smiles_length': len(s),\n",
    "        'smiles_num_atoms': sum(c.isupper() for c in s),\n",
    "        'smiles_num_bonds': s.count('=') + s.count('#'),\n",
    "        'smiles_num_rings': s.count('1') + s.count('2'),\n",
    "        'smiles_has_aromatic': int('c' in s or 'n' in s or 'o' in s or 's' in s),\n",
    "        'smiles_num_branches': s.count('('),\n",
    "        'smiles_num_double_bonds': s.count('='),\n",
    "        'smiles_num_triple_bonds': s.count('#'),\n",
    "    }\n",
    "\n",
    "smiles_features = df['smiles_smiles'].apply(extract_smiles_features)\n",
    "smiles_df = pd.DataFrame(smiles_features.tolist())\n",
    "\n",
    "print(f\"SMILES features: {smiles_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b4d89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Molecular descriptors\n",
    "molecular_cols = [\n",
    "    'molecular_weight', 'n_hba', 'n_hbd', \n",
    "    'logp', 'logp_ali', 'logp_cyclo', \n",
    "    'pka_acidic', 'pka_basic', \n",
    "    'solubility', 'ro5_fulfilled'\n",
    "]\n",
    "\n",
    "available_molecular = [col for col in molecular_cols if col in df.columns]\n",
    "molecular_df = df[available_molecular].copy()\n",
    "\n",
    "# Fill missing values with median\n",
    "for col in molecular_df.columns:\n",
    "    if molecular_df[col].isnull().any():\n",
    "        median_val = molecular_df[col].median()\n",
    "        molecular_df[col].fillna(median_val, inplace=True)\n",
    "\n",
    "print(f\"Molecular features: {molecular_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928c28a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create categorical encodings for embeddings (we'll use these differently)\n",
    "# Encode drug names as integers for embedding layer\n",
    "drug_encoder = LabelEncoder()\n",
    "df['drug_id'] = drug_encoder.fit_transform(df['chemicalname'].astype(str))\n",
    "num_drugs = len(drug_encoder.classes_)\n",
    "\n",
    "print(f\"Unique drugs for embedding: {num_drugs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213e9b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine continuous features\n",
    "X_continuous = pd.concat([\n",
    "    tfidf_df.reset_index(drop=True),\n",
    "    smiles_df.reset_index(drop=True),\n",
    "    molecular_df.reset_index(drop=True),\n",
    "], axis=1)\n",
    "\n",
    "# Separate categorical feature\n",
    "X_categorical = df[['drug_id']].values\n",
    "\n",
    "# Target\n",
    "y = df['diseasename']\n",
    "\n",
    "print(f\"Continuous features: {X_continuous.shape[1]}\")\n",
    "print(f\"Categorical features: {X_categorical.shape[1]}\")\n",
    "print(f\"Target classes: {y.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc17fbea",
   "metadata": {},
   "source": [
    "## 3. Train/Val/Test Split with Class Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb2788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter classes with at least 3 samples (need more for better training)\n",
    "class_counts = y.value_counts()\n",
    "valid_classes = class_counts[class_counts >= 3].index\n",
    "\n",
    "mask = y.isin(valid_classes)\n",
    "X_cont_filtered = X_continuous[mask].reset_index(drop=True)\n",
    "X_cat_filtered = X_categorical[mask]\n",
    "y_filtered = y[mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"Filtered dataset: {len(X_cont_filtered)} samples\")\n",
    "print(f\"Valid classes: {len(valid_classes)}\")\n",
    "\n",
    "# Encode target\n",
    "le_target = LabelEncoder()\n",
    "y_encoded = le_target.fit_transform(y_filtered)\n",
    "num_classes = len(le_target.classes_)\n",
    "\n",
    "# Calculate class weights for balanced training\n",
    "class_sample_counts = np.bincount(y_encoded)\n",
    "class_weights = 1.0 / class_sample_counts\n",
    "class_weights = class_weights / class_weights.sum() * num_classes  # Normalize\n",
    "sample_weights = class_weights[y_encoded]\n",
    "\n",
    "print(f\"Class weights calculated for {num_classes} classes\")\n",
    "\n",
    "# 60/20/20 split\n",
    "X_cont_temp, X_cont_test, X_cat_temp, X_cat_test, y_temp, y_test, weights_temp, weights_test = train_test_split(\n",
    "    X_cont_filtered, X_cat_filtered, y_encoded, sample_weights,\n",
    "    test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "X_cont_train, X_cont_val, X_cat_train, X_cat_val, y_train, y_val, weights_train, weights_val = train_test_split(\n",
    "    X_cont_temp, X_cat_temp, y_temp, weights_temp,\n",
    "    test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain: {len(X_cont_train)} | Val: {len(X_cont_val)} | Test: {len(X_cont_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85133e01",
   "metadata": {},
   "source": [
    "## 4. Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d867e3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize continuous features\n",
    "scaler = StandardScaler()\n",
    "X_cont_train_scaled = scaler.fit_transform(X_cont_train)\n",
    "X_cont_val_scaled = scaler.transform(X_cont_val)\n",
    "X_cont_test_scaled = scaler.transform(X_cont_test)\n",
    "\n",
    "print(\"Features standardized (mean=0, std=1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebda6222",
   "metadata": {},
   "source": [
    "## 5. PyTorch Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83b6fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugDiseaseDataset(Dataset):\n",
    "    def __init__(self, X_continuous, X_categorical, y):\n",
    "        self.X_cont = torch.FloatTensor(X_continuous)\n",
    "        self.X_cat = torch.LongTensor(X_categorical)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_cont[idx], self.X_cat[idx], self.y[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DrugDiseaseDataset(X_cont_train_scaled, X_cat_train, y_train)\n",
    "val_dataset = DrugDiseaseDataset(X_cont_val_scaled, X_cat_val, y_val)\n",
    "test_dataset = DrugDiseaseDataset(X_cont_test_scaled, X_cat_test, y_test)\n",
    "\n",
    "# Create weighted sampler for balanced training\n",
    "train_sampler = WeightedRandomSampler(\n",
    "    weights=weights_train,\n",
    "    num_samples=len(weights_train),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"DataLoaders created with batch size: {batch_size}\")\n",
    "print(f\"Train batches: {len(train_loader)} | Val batches: {len(val_loader)} | Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be68185",
   "metadata": {},
   "source": [
    "## 6. Advanced Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71b3628",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedDrugDiseaseNet(nn.Module):\n",
    "    def __init__(self, num_continuous, num_drugs, num_classes, \n",
    "                 embedding_dim=32, hidden_dims=[512, 256, 128], dropout=0.3):\n",
    "        super(AdvancedDrugDiseaseNet, self).__init__()\n",
    "        \n",
    "        # Embedding layer for drug IDs\n",
    "        self.drug_embedding = nn.Embedding(num_drugs, embedding_dim)\n",
    "        \n",
    "        # Input dimension = continuous features + embedding dimension\n",
    "        input_dim = num_continuous + embedding_dim\n",
    "        \n",
    "        # Build deep network with BatchNorm and Dropout\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        self.hidden_layers = nn.Sequential(*layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(prev_dim, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Embedding):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "    \n",
    "    def forward(self, X_cont, X_cat):\n",
    "        # Get drug embeddings\n",
    "        drug_emb = self.drug_embedding(X_cat.squeeze())\n",
    "        \n",
    "        # Concatenate continuous features with embeddings\n",
    "        x = torch.cat([X_cont, drug_emb], dim=1)\n",
    "        \n",
    "        # Pass through hidden layers\n",
    "        x = self.hidden_layers(x)\n",
    "        \n",
    "        # Output logits\n",
    "        logits = self.output_layer(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create model\n",
    "model = AdvancedDrugDiseaseNet(\n",
    "    num_continuous=X_cont_train_scaled.shape[1],\n",
    "    num_drugs=num_drugs,\n",
    "    num_classes=num_classes,\n",
    "    embedding_dim=32,\n",
    "    hidden_dims=[512, 256, 128, 64],\n",
    "    dropout=0.4\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58bd407",
   "metadata": {},
   "source": [
    "## 7. Training Setup with Advanced Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01326952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function with label smoothing\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        n_classes = pred.size(-1)\n",
    "        log_probs = F.log_softmax(pred, dim=-1)\n",
    "        \n",
    "        # One-hot with smoothing\n",
    "        true_dist = torch.zeros_like(log_probs)\n",
    "        true_dist.fill_(self.smoothing / (n_classes - 1))\n",
    "        true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "        \n",
    "        return torch.mean(torch.sum(-true_dist * log_probs, dim=-1))\n",
    "\n",
    "criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
    "\n",
    "# Optimizer with weight decay\n",
    "optimizer = AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "print(\"Training setup complete:\")\n",
    "print(f\"  - Loss: Label Smoothing Cross Entropy (smoothing=0.1)\")\n",
    "print(f\"  - Optimizer: AdamW (lr=0.001, weight_decay=0.01)\")\n",
    "print(f\"  - Scheduler: Cosine Annealing with Warm Restarts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24babebc",
   "metadata": {},
   "source": [
    "## 8. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82845de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X_cont, X_cat, y in loader:\n",
    "        X_cont, X_cat, y = X_cont.to(device), X_cat.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_cont, X_cat)\n",
    "        loss = criterion(outputs, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += y.size(0)\n",
    "        correct += predicted.eq(y).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion, device, k_values=[1, 5, 10, 20]):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_cont, X_cat, y in loader:\n",
    "            X_cont, X_cat, y = X_cont.to(device), X_cat.to(device), y.to(device)\n",
    "            \n",
    "            outputs = model(X_cont, X_cat)\n",
    "            loss = criterion(outputs, y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            all_outputs.append(outputs.cpu())\n",
    "            all_targets.append(y.cpu())\n",
    "    \n",
    "    all_outputs = torch.cat(all_outputs)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "    \n",
    "    # Calculate top-k accuracies\n",
    "    results = {'loss': total_loss / len(loader)}\n",
    "    \n",
    "    for k in k_values:\n",
    "        _, top_k_preds = all_outputs.topk(k, dim=1)\n",
    "        correct = sum(all_targets[i] in top_k_preds[i] for i in range(len(all_targets)))\n",
    "        results[f'top{k}'] = 100. * correct / len(all_targets)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef823f21",
   "metadata": {},
   "source": [
    "## 9. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6723d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "best_val_top20 = 0\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_top1': [], 'val_top5': [], 'val_top10': [], 'val_top20': []\n",
    "}\n",
    "\n",
    "print(f\"Starting training for {num_epochs} epochs...\")\n",
    "print(f\"Early stopping patience: {patience}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_results = evaluate(model, val_loader, criterion, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_results['loss'])\n",
    "    history['val_top1'].append(val_results['top1'])\n",
    "    history['val_top5'].append(val_results['top5'])\n",
    "    history['val_top10'].append(val_results['top10'])\n",
    "    history['val_top20'].append(val_results['top20'])\n",
    "\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Train - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val   - Loss: {val_results['loss']:.4f}\")\n",
    "        print(\n",
    "            f\"  Val   - Top-1: {val_results['top1']:.2f}%, Top-5: {val_results['top5']:.2f}%, \"\n",
    "            f\"Top-10: {val_results['top10']:.2f}%, Top-20: {val_results['top20']:.2f}%\"\n",
    "        )\n",
    "        print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        print(\"-\"*80)\n",
    "\n",
    "    if val_results['top20'] > best_val_top20:\n",
    "        best_val_top20 = val_results['top20']\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        patience_counter = 0\n",
    "        print(f\"  Stored checkpoint with Top-20 = {best_val_top20:.2f}%\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training complete!\")\n",
    "print(f\"Best validation Top-20 accuracy: {best_val_top20:.2f}%\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730cf2c4",
   "metadata": {},
   "source": [
    "## 10. Evaluate Best Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e9c9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = evaluate(model, test_loader, criterion, device, k_values=[1, 5, 10, 20, 50])\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL TEST SET RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Test Loss: {test_results['loss']:.4f}\")\n",
    "print(f\"\\nTop-K Accuracies:\")\n",
    "print(f\"  Top-1:  {test_results['top1']:.2f}%\")\n",
    "print(f\"  Top-5:  {test_results['top5']:.2f}%\")\n",
    "print(f\"  Top-10: {test_results['top10']:.2f}%\")\n",
    "print(f\"  Top-20: {test_results['top20']:.2f}%\")\n",
    "print(f\"  Top-50: {test_results['top50']:.2f}%\")\n",
    "\n",
    "# Compare with random baseline\n",
    "random_top1 = 100 / num_classes\n",
    "random_top20 = 2000 / num_classes\n",
    "\n",
    "print(f\"\\nImprovement over Random Baseline:\")\n",
    "print(f\"  Top-1:  {test_results['top1']/random_top1:.2f}x better\")\n",
    "print(f\"  Top-20: {test_results['top20']/random_top20:.2f}x better\")\n",
    "\n",
    "# Compare with LightGBM baseline\n",
    "lgb_top1 = 0.66\n",
    "lgb_top20 = 14.55\n",
    "\n",
    "print(f\"\\nComparison with LightGBM Baseline:\")\n",
    "print(f\"  Top-1:  {test_results['top1']/lgb_top1:.2f}x {'better' if test_results['top1'] > lgb_top1 else 'worse'}\")\n",
    "print(f\"  Top-20: {test_results['top20']/lgb_top20:.2f}x {'better' if test_results['top20'] > lgb_top20 else 'worse'}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faecef97",
   "metadata": {},
   "source": [
    "## 11. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f4e91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0, 0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(epochs, history['val_top1'], 'g-', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Top-1 Accuracy (%)', fontsize=12)\n",
    "axes[0, 1].set_title('Validation Top-1 Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(epochs, history['val_top1'], label='Top-1', linewidth=2)\n",
    "axes[1, 0].plot(epochs, history['val_top5'], label='Top-5', linewidth=2)\n",
    "axes[1, 0].plot(epochs, history['val_top10'], label='Top-10', linewidth=2)\n",
    "axes[1, 0].plot(epochs, history['val_top20'], label='Top-20', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1, 0].set_title('Validation Top-K Accuracies', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(epochs, history['val_top20'], 'purple', linewidth=3)\n",
    "axes[1, 1].axhline(y=lgb_top20, color='red', linestyle='--', label='LightGBM Baseline', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Top-20 Accuracy (%)', fontsize=12)\n",
    "axes[1, 1].set_title('Top-20 Accuracy vs LightGBM Baseline', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Training history visualized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08fcaf3",
   "metadata": {},
   "source": [
    "## 12. Analyze Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27241ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on test set\n",
    "model.eval()\n",
    "all_probs = []\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_cont, X_cat, y in test_loader:\n",
    "        X_cont, X_cat = X_cont.to(device), X_cat.to(device)\n",
    "        outputs = model(X_cont, X_cat)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        \n",
    "        all_probs.append(probs.cpu().numpy())\n",
    "        all_preds.append(outputs.argmax(dim=1).cpu().numpy())\n",
    "        all_targets.append(y.numpy())\n",
    "\n",
    "all_probs = np.vstack(all_probs)\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "\n",
    "# Confidence analysis\n",
    "max_probs = all_probs.max(axis=1)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PREDICTION CONFIDENCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Mean confidence: {max_probs.mean():.4f}\")\n",
    "print(f\"Median confidence: {np.median(max_probs):.4f}\")\n",
    "print(f\"% predictions > 0.5 confidence: {(max_probs > 0.5).mean()*100:.2f}%\")\n",
    "print(f\"% predictions > 0.9 confidence: {(max_probs > 0.9).mean()*100:.2f}%\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb417f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(max_probs, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "plt.xlabel('Prediction Confidence (Max Probability)', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.title('Distribution of Prediction Confidences on Test Set', fontsize=14, fontweight='bold')\n",
    "plt.axvline(max_probs.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {max_probs.mean():.3f}')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5916e4c",
   "metadata": {},
   "source": [
    "## 13. Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87588bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get classification report\n",
    "report = classification_report(all_targets, all_preds, output_dict=True, zero_division=0)\n",
    "\n",
    "# Extract F1 scores\n",
    "class_f1_scores = []\n",
    "for i in range(num_classes):\n",
    "    if str(i) in report:\n",
    "        class_f1_scores.append(report[str(i)]['f1-score'])\n",
    "    else:\n",
    "        class_f1_scores.append(0.0)\n",
    "\n",
    "class_f1_scores = np.array(class_f1_scores)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PER-CLASS PERFORMANCE (F1 SCORES)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total classes: {num_classes}\")\n",
    "print(f\"Classes with F1 > 0: {(class_f1_scores > 0).sum()} ({(class_f1_scores > 0).sum()/num_classes*100:.1f}%)\")\n",
    "print(f\"Classes with F1 >= 0.5: {(class_f1_scores >= 0.5).sum()} ({(class_f1_scores >= 0.5).sum()/num_classes*100:.1f}%)\")\n",
    "print(f\"Mean F1 Score: {class_f1_scores.mean():.4f}\")\n",
    "print(f\"Median F1 Score: {np.median(class_f1_scores):.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf9c160",
   "metadata": {},
   "source": [
    "## 14. Final Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1fb2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Phase 3: Final Summary\".center(100))\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\nMODEL ARCHITECTURE:\")\n",
    "print(\"   • Deep neural network with hidden layers [512, 256, 128, 64]\")\n",
    "print(\"   • 32-dimensional drug embedding layer\")\n",
    "print(\"   • Batch Normalization + Dropout (0.4) for regularization\")\n",
    "print(f\"   • Total parameters: {total_params:,}\")\n",
    "\n",
    "print(\"\\nADVANCED TRAINING TECHNIQUES:\")\n",
    "print(\"   • Label Smoothing (0.1)\")\n",
    "print(\"   • AdamW optimizer with weight decay\")\n",
    "print(\"   • Cosine Annealing learning-rate schedule\")\n",
    "print(\"   • Weighted sampling for class balance\")\n",
    "print(\"   • Gradient clipping (max_norm=1.0)\")\n",
    "print(\"   • Early stopping (patience=15)\")\n",
    "\n",
    "print(\"\\nTEST SET RESULTS:\")\n",
    "print(f\"   Top-1 Accuracy:  {test_results['top1']:.2f}%\")\n",
    "print(f\"   Top-5 Accuracy:  {test_results['top5']:.2f}%\")\n",
    "print(f\"   Top-10 Accuracy: {test_results['top10']:.2f}%\")\n",
    "print(f\"   Top-20 Accuracy: {test_results['top20']:.2f}%\")\n",
    "print(f\"   Top-50 Accuracy: {test_results['top50']:.2f}%\")\n",
    "\n",
    "print(\"\\nCOMPARISON WITH BASELINES:\")\n",
    "print(\n",
    "    f\"   Random (Top-1):  {random_top1:.2f}% → {test_results['top1']:.2f}% \"\n",
    "    f\"({test_results['top1']/random_top1:.2f}x improvement)\"\n",
    ")\n",
    "print(\n",
    "    f\"   Random (Top-20): {random_top20:.2f}% → {test_results['top20']:.2f}% \"\n",
    "    f\"({test_results['top20']/random_top20:.2f}x improvement)\"\n",
    ")\n",
    "print(\n",
    "    f\"   LightGBM (Top-1):  {lgb_top1:.2f}% → {test_results['top1']:.2f}% \"\n",
    "    f\"({'+' if test_results['top1'] > lgb_top1 else '-'}{abs(test_results['top1']-lgb_top1):.2f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"   LightGBM (Top-20): {lgb_top20:.2f}% → {test_results['top20']:.2f}% \"\n",
    "    f\"({'+' if test_results['top20'] > lgb_top20 else '-'}{abs(test_results['top20']-lgb_top20):.2f}%)\"\n",
    ")\n",
    "\n",
    "improvement_top1 = ((test_results['top1'] - lgb_top1) / lgb_top1) * 100\n",
    "improvement_top20 = ((test_results['top20'] - lgb_top20) / lgb_top20) * 100\n",
    "\n",
    "if test_results['top20'] > lgb_top20:\n",
    "    print(f\"\\nRESULT: Neural network exceeds LightGBM Top-20 by {improvement_top20:.1f}%.\")\n",
    "else:\n",
    "    print(f\"\\nRESULT: Neural network trails LightGBM Top-20 by {abs(improvement_top20):.1f}%.\")\n",
    "    print(\"   Possible causes include limited class support, sparse TF-IDF signals, or strong tree baselines.\")\n",
    "\n",
    "print(\"\\nMODEL DIAGNOSTICS:\")\n",
    "print(f\"   Mean prediction confidence: {max_probs.mean():.3f}\")\n",
    "print(f\"   Classes with non-zero F1: {(class_f1_scores > 0).sum()} / {num_classes}\")\n",
    "print(f\"   Classes with F1 ≥ 0.5: {(class_f1_scores >= 0.5).sum()} / {num_classes}\")\n",
    "\n",
    "print(\"\\nRECOMMENDATIONS FOR REPORTING:\")\n",
    "if test_results['top20'] > lgb_top20:\n",
    "    print(\"   • Present this neural model as the primary Phase 3 result.\")\n",
    "    print(\"   • Emphasize the role of embeddings, balanced sampling, and LR scheduling.\")\n",
    "else:\n",
    "    print(\"   • Present this model as a complementary experiment alongside LightGBM.\")\n",
    "    print(\"   • Discuss why tree-based models can dominate sparse tabular settings.\")\n",
    "\n",
    "print(\"\\nFUTURE IMPROVEMENT IDEAS:\")\n",
    "print(\"   • Increase embedding dimensionality (e.g., 64 or 128).\")\n",
    "print(\"   • Explore attention mechanisms or cross-modal encoders.\")\n",
    "print(\"   • Build ensembles combining neural and tree-based predictions.\")\n",
    "print(\"   • Incorporate pre-trained molecular representations (ChemBERTa, MolBERT).\")\n",
    "print(\"   • Expand the dataset with additional curated indications.\")\n",
    "\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ce5fe0",
   "metadata": {},
   "source": [
    "## Phase 3 Presentation Visuals\n",
    "All visuals below are generated purely from Phase 3 data and metrics captured in this notebook so they can be dropped straight into your slides without referencing earlier phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4817c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "FIG_DIR = Path(\"figures\")\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"deep\")\n",
    "print(f\"Figures will be saved to: {FIG_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f04e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 15\n",
    "\n",
    "if 'y_filtered' in globals():\n",
    "    class_series = pd.Series(y_filtered)\n",
    "elif 'df' in globals():\n",
    "    class_series = df['diseasename']\n",
    "else:\n",
    "    df = pd.read_csv('finaldataset.csv', on_bad_lines='skip', engine='python')\n",
    "    class_series = df['diseasename']\n",
    "\n",
    "top_classes = class_series.value_counts().head(top_k).sort_values()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "sns.barplot(x=top_classes.values, y=top_classes.index, ax=ax, palette=\"viridis\")\n",
    "ax.set_title(f\"Top {top_k} Disease Classes in Phase 3 Dataset\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"Samples\")\n",
    "ax.set_ylabel(\"Disease Name\")\n",
    "for i, value in enumerate(top_classes.values):\n",
    "    ax.text(value + 5, i, f\"{value}\", va=\"center\", fontsize=9)\n",
    "\n",
    "fig.tight_layout()\n",
    "class_fig_path = FIG_DIR / \"phase3_class_distribution_top15.png\"\n",
    "fig.savefig(class_fig_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved class distribution chart to {class_fig_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1235a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].plot(epochs, history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(epochs, history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_title('Phase 3 Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "for metric, color in zip(['val_top1', 'val_top5', 'val_top10', 'val_top20'], ['#1d3557', '#2a9d8f', '#e9c46a', '#f4a261']):\n",
    "    axes[1].plot(epochs, history[metric], label=metric.replace('val_', '').upper(), linewidth=2, color=color)\n",
    "axes[1].set_title('Validation Top-K Accuracy Progression', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "fig.tight_layout()\n",
    "training_fig_path = FIG_DIR / 'phase3_training_curves.png'\n",
    "fig.savefig(training_fig_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved training curves to {training_fig_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cce5456",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_labels = ['Top-1', 'Top-5', 'Top-10', 'Top-20', 'Top-50']\n",
    "topk_values = [test_results['top1'], test_results['top5'], test_results['top10'], test_results['top20'], test_results['top50']]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sns.barplot(x=topk_labels, y=topk_values, ax=ax, palette='Blues_d')\n",
    "ax.set_ylim(0, max(topk_values) + 10)\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('Phase 3 Test Top-K Accuracy', fontsize=14, fontweight='bold')\n",
    "for label, value in zip(topk_labels, topk_values):\n",
    "    ax.text(label, value + 1.5, f\"{value:.2f}%\", ha='center', fontsize=10)\n",
    "\n",
    "fig.tight_layout()\n",
    "topk_fig_path = FIG_DIR / 'phase3_test_topk.png'\n",
    "fig.savefig(topk_fig_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved Top-K accuracy bars to {topk_fig_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14758da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "sns.histplot(max_probs, bins=40, kde=True, color='#457b9d', ax=ax)\n",
    "ax.set_title('Model Prediction Confidence (Phase 3 Test Set)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Maximum Softmax Probability')\n",
    "ax.set_ylabel('Count')\n",
    "ax.axvline(max_probs.mean(), color='red', linestyle='--', label=f\"Mean = {max_probs.mean():.2f}\")\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.2)\n",
    "\n",
    "conf_fig_path = FIG_DIR / 'phase3_prediction_confidence.png'\n",
    "fig.savefig(conf_fig_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\">50% confidence: {(max_probs > 0.5).mean()*100:.2f}%  |  >90% confidence: {(max_probs > 0.9).mean()*100:.2f}%\")\n",
    "print(f\"Saved confidence histogram to {conf_fig_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb7ce00",
   "metadata": {},
   "source": [
    "_Each figure is written to `figures/` with a descriptive filename (`phase3_class_distribution_top15.png`, `phase3_training_curves.png`, `phase3_test_topk.png`, `phase3_prediction_confidence.png`). Run these cells after training/evaluation, then drop the PNGs directly into your Phase 3 slides._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
